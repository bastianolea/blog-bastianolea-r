---
title: 'Tutorial: web scraping de sitios web con R usando {rvest}'
author: Basti√°n Olea Herrera
date: '2024-12-27'
draft: false
format: 
  hugo-md:
    output-file: "index"
    output-ext:  "md"
slug: []
categories:
  - tutoriales
tags:
  - web scraping
  - datos
execute: 
  cache: true
editor_options: 
  chunk_output_type: console
---


Se denomina web scraping a un conjunto de t√©cnicas usadas para **obtener datos desde p√°ginas web**. Esto significa poder transformar la informaci√≥n que vemos en distintos sitios de internet en datos que podamos utilizar. 

Se usa el web scraping cuando un sitio web presenta informaci√≥n, cifras, datos, n√∫meros, o cualquier otro elemento que nos pueda servir, pero sin facilitar acceso a los datos, como ser√≠a un enlace de descarga, una API para obtener los datos, o alguna forma de exportar la informaci√≥n. En estos casos tenemos que recurrir al scraping para transformar lo que vemos en la web en datos analizables.

En este tutorial aprenderemos a hacer web scraping en R para **extraer cualquier informaci√≥n que veamos en un sitio web**, y traerla a nuestro entorno de R para poder procesarla como deseemos.

## Web scraping con {rvest}

La primera opci√≥n a la hora de hacer web scraping de sitios web con R es [el paquete {rvest}](https://rvest.tidyverse.org) (_harvest,_ o _cosechar_ en espa√±ol). Este paquete, parte del [Tidyverse](https://www.tidyverse.org/), suele ser la opci√≥n m√°s sencilla, m√°s popular y mejor documentada para extraer datos desde sitios web est√°ticos en R.

```{r}
# install.packages("rvest")
library(rvest)
```


Existen otros paquetes para el web scraping, como [{RSelenium}](https://docs.ropensci.org/RSelenium/index.html), que se caracteriza por permitirnos obtener datos desde sitios web din√°micos, pero a su vez es menos intuitivo y m√°s dif√≠cil de configurar, lo que hace que sea necesario explicarlo en un tutorial aparte.


### Extraer tablas desde un sitio web

Como primer paso, extraeremos los datos de una tabla alojada en internet. Cuando las tablas est√°n formateadas apropiadamente, como las tablas que podemos encontrar en Wikipedia, extraerlas resulta muy sencillo.

El primer paso es definir la direcci√≥n del sitio que queremos scrapear, y luego usamos dos funciones para obtener el c√≥digo de fuente de este sitio: la funci√≥n `session()`, que nos permite conectarnos al sitio web creando una _sesi√≥n_, y luego la funci√≥n `read_html()`, que descargar√° el c√≥digo del sitio y nos permitir√° extraer informaci√≥n desde el mismo.

```{r tabla_1}
url <- "https://es.wikipedia.org/wiki/Chile"

sitio_chile <- session(url) |> 
  read_html()

sitio_chile
```

El objeto resultante es un documento HTML del cual podremos extraer los elementos del sitio que nos interesen.

Intentemos extraer la tabla del sitio que posee datos sobre poblaci√≥n ind√≠gena en el pa√≠s:

![](scraping_1.jpeg)

Para extraer las tablas de este sitio, usamos la funci√≥n `html_table()`. Las funciones que extraen elementos de los sitios empiezan con `html`.

```{r tabla_2}
tablas <- sitio_chile |> 
  html_table()
```

El resultado de esta extracci√≥n va a ser una lista. En R, una lista es un tipo de objeto que dentro de s√≠ puede contener m√∫ltiples elementos de distinto tipo, forma, o tama√±o. En este caso, esta lista contiene todas las tablas que hab√≠an en la p√°gina. Entonces, tenemos que elegir la tabla que nos interesa analizar. 

En este caso, nos interesa la octava tabla, que contiene informaci√≥n sobre pueblos ind√≠genas en Chile, por lo que la extraemos de la lista:

```{r tabla_3}
# extraer tabla de la lista
tabla_1 <- tablas[[8]]

tabla_1

# limpiar datos de la tabla
library(dplyr)

# renombrar columnas
tabla_1a <- tabla_1 |> 
  janitor::row_to_names(1) |> 
  janitor::clean_names()

# convertir variables a num√©ricas
tabla_1b <- tabla_1a |> 
  # remover espacios
  mutate(poblacion = stringr::str_remove_all(poblacion, "\\s+")) |>  
  # reemplazar comas por puntos
  mutate(percent = stringr::str_replace(percent, ",", ".")) |> 
  # convertir a num√©ricas
  mutate(across(c(poblacion, percent), as.numeric))

tabla_1b
```

¬°Listo! Extrajimos datos desde una tabla de Wikipedia, y ahora podemos usarlos para lo que necesitemos.


### Extraer datos de un sitio usando etiquetas

Para scrapear elementos puntuales de un sitio, necesitamos identificarlos, y as√≠ extraer solamente lo que nos interesa. Cada elemento en un sitio web, ya sea texto, tablas o im√°genes, se crea a partir de [etiquetas HTML](https://developer.mozilla.org/es/docs/Web/HTML/Element). Si identificamos las etiquetas HTML de los datos que necesitamos, podemos usarlas para extraerlos.

Probemos con otra p√°gina de Wikipedia: 

![](scraping_2.jpeg)

```{r sitio_1}
url <- "https://es.wikipedia.org/wiki/Mapache"

sitio_mapache <- session(url) |> 
  read_html()
```

Para extraer elementos de un sitio, usamos la funci√≥n `html_elements()`, cuyo argumento ser√° el identificador del elemento que queremos extraer. Para extraer el t√≠tulo de la p√°gina, usualmente podemos usar la etiqueta HTML `h1`, que se usa para t√≠tulos de mayor jerarqu√≠a (existen etiquetas de t√≠tulos `h1`, `h2`... hasta `h6`).

```{r sitio_2}
sitio_mapache |> 
  html_elements("h1")
```

Obtenemos un elemento que empieza con `<h1...`, lo que significa que extrajimos un elemento en el sitio con dicha etiqueta `h1`. Para convertir este elemento a texto, usamos la funci√≥n `html_text()`:

```{r sitio_3}
sitio_mapache |> 
  html_elements("h1") |> 
  html_text()
```

Del mismo modo, podemos extraer otros elementos del sitio, tales como los subt√≠tulos de etiqueta `h2`:

```{r sitio_4}
sitio_mapache |> 
  html_elements("h2") |> 
  html_text()
```

Si deseamos extraer todo el texto del sitio, apuntamos a la etiqueta de p√°rrafo, que es `p`:

```{r sitio_5}
texto <- sitio_mapache |> 
  html_elements("p") |> 
  html_text()

texto[3:5]
```

El objeto resultante es un vector de texto, separado en p√°rrafos.


### Extraer datos de un sitio usando clases

Hagamos otro ejemplo sobre un sitio menos estructurado que Wikipedia: queremos obtener una lista de animales end√©micos de Chile desde el sitio web [animalia.bio](https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile). 

En este sitio, cada t√≠tuar de los p√°rrafos contiene el nombre de un animal end√©mico. Por alguna raz√≥n, este sitio impide que le hagamos scraping si nos conectamos con la funci√≥n `session()`[^1], pero tambi√©n es posible extraer el c√≥digo directamente del sitio, sin crear una sesi√≥n. Cu√°ndo se hace web scraping es posible encontrarse con problemas de este tipo, como sitios que evitan entregar su informaci√≥n, o que lo hacen un poco m√°s complicado de lo normal.

[^1]: Lo m√°s probable sea que este sitio identifique que estamos conect√°ndonos desde un paquete de R que hace web scraping, y por lo tanto cancela la conexi√≥n entregando un error 403 (prohibido). Esto se debe a que cualquier interfaz que se conecte una p√°gina web entrega un _user agent_ que los identifica, y muchos sitios usan esta informaci√≥n para prohibir el acceso o mostrar distintos elementos dependiendo del navegador que uses.

```{r sitio_6}
url <- "https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile"

# extraer c√≥digo de fuente del sitio sin abrir una sesi√≥n
sitio_animales <- read_html(url)

sitio_animales
```

Con este sitio web no nos resulta extraer la etiqueta `h2`, `h3` o `h4` que se usan usualmente para los titulares en sus distintos niveles, sencillamente porque este sitio web no usa esas etiquetas para crear sus titulares. 

```{r sitio_7}
sitio_animales |> 
  html_elements("h2")
```

En estos casos, tenemos que entrar al sitio web e identificar manualmente c√≥mo se distinguen entre el resto del c√≥digo los elementos que queremos extraer. 

Accedemos al sitio web con un navegador web cualquiera, y entramos al inspector web de nuestro navegador. Podemos hacer clic derecho en alg√∫n elemento del sitio, y elegir la opci√≥n _Inspeccionar_ para abrir el inspector:

![](scraping_3.jpeg)

Se abrir√° un **inspector web** donde veremos el c√≥digo de fuente del sitio al lado del sitio mismo. Si movemos nuestro cursor sobre las l√≠neas del c√≥digo, se destacar√°n los elementos del sitio web que corresponden a cada l√≠nea, o viceversa. De esta forma, podemos encontrar exactamente cu√°l l√≠nea de c√≥digo se corresponde con el elemento que queremos extraer.

![](scraping_4.jpeg)

En este caso, la l√≠nea de c√≥digo de los titulares de cada animal es a una etiqueta `span`, que corresponde a un contenedor de texto. Por s√≠ sola, esta etiqueta no identifica de manera √∫nica a los datos que queremos extraer, por lo tanto, tenemos que encontrar otro identificador dentro de esta etiqueta que nos sirva. 

En la mayor√≠a de los casos, los elementos de un sitio web que comparten un mismo estilo gr√°fico (tipograf√≠a, tama√±o de texto, color, etc.) comparten tambi√©n una **clase CSS**. Una clase CSS es una forma de definir la apariencia de una parte de un sitio web, como un titular, un p√°rrafo de texto, o un bot√≥n, definiendo su apariencia en una hoja de estilos CSS a partir del nombre de la clase, para luego aplicar esta misma clase a m√∫ltiples elementos del sitio. Por lo tanto, **identificar la clase de un elemento web usualmente nos permite extraer m√∫ltiples elementos de un sitio que comparten una misma jerarqu√≠a o apariencia.** 

En el caso de este sitio, y como vemos en el c√≥digo del inspector web de nuestro navegador, todos los t√≠tulos de los animales del sitio poseen la clase `collection-animal-title`. Entonces, podemos usar esa clase para extraer los elementos usando la funci√≥n `html_elements()`. Pero hay que tener en consideraci√≥n que las clases CSS se escriben anteponi√©ndoles un punto, por lo que usamos `".collection-animal-title"`:

```{r sitio_8}
animales <- sitio_animales |> 
  html_elements(".collection-animal-title") |> 
  html_text()

animales[1:10]
```

Al extraer los elementos que comparten una misma clase, `{rvest}` nos entrega un vector que contiene todos elementos del sitio que usan en esa clase; en este caso, los t√≠tulos de los animales.

Si seguimos viendo el c√≥digo de fuente del sitio, notamos que tambi√©n hay otras clases que describen los elementos asociados a cada animal: la clase `collection-desc` que se aplica a los p√°rrafos, y la clase `animal-link` que contiene el enlace a cada animal.

Para extraer los textos, apuntamos a la clase `.collection-desc`, y usamos `html_text2()`, que adem√°s de convertir a texto, tambi√©n ayuda removiendo caracteres en blanco como espacios o saltos de l√≠nea:

```{r  sitio_9}
textos <- sitio_animales |> 
  html_elements(".collection-desc") |> 
  html_text2()

textos[1:5]
```

Para extraer los enlaces, el proceso es levemente distinto, porque si extraemos el texto de los enlaces desde la clase `.animal-link`, solamente obtendremos el texto, no el enlace en s√≠ mismo:

```{r sitio_10a}
sitio_animales |> 
  html_elements(".animal-link") |> 
  html_text()
```

Notamos que la etiqueta HTML que se usa para crear los enlaces es la etiqueta `a`, y dentro de esta etiqueta, adem√°s de la clase tenemos otros **atributos**, tales como el atributo `href` (_hypertext reference_), que contiene la direcci√≥n a la que apunta el enlace:

```{r sitio_10b}
sitio_animales |> 
  html_elements(".animal-link")
```

Para extraer este atributo de los elementos de etiqueta `a`, usamos la funci√≥n `html_attr()`, especificando el atributo que queremos extraer:

```{r sitio_10c}
enlaces <- sitio_animales |> 
  html_elements(".animal-link") |> 
  html_attr("href")

enlaces[1:10]
```

Una vez que hayamos extra√≠do los elementos del sitio que necesitamos, y considerando que los elementos extra√≠dos tienen todos el mismo largo (dado que cada animal del sitio ten√≠a exactamente un t√≠tulo, un p√°rrafo y un enlace), podemos unir todos los resultados en una tabla para **construir un dataframe a partir de los datos scrapeados.**

```{r}
tabla_animales <- tibble(animales,
                         textos, 
                         enlaces)

tabla_animales
```

Creamos una tabla con los resultados de los tres datos que scrapeamos! 

En general, √©sta es la l√≥gica b√°sica del web scraping. Se debe identificar el sitio web que necesitas, luego explorar el c√≥digo de fuente del sitio para entender cu√°les son las clases de los elementos que quieres extraer, para finalmente extraerlos y procesarlos, de ser necesario. En muchos casos, lo que se hace es obtener una cierta cantidad de enlaces que contienen informaci√≥n que est√° estructurada de una manera homog√©nea, y luego se ejecuta el proceso de web scraping a cada uno de los enlaces a trav√©s de un loop o una iteraci√≥n.

----

Si te sirvi√≥ este tutorial, por favor considera hacerme una peque√±a donaci√≥n para poder tomarme un cafecito mientras escribo el siguiente tutorial ü•∫

```{=html}
<div style = "height: 18px;">
</div>
<div>
  <div style="display: flex;
  justify-content: center;
  align-items: center;">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="bastimapache" data-color="#FFDD00" data-emoji="‚òï"  data-font="Cookie" data-text="Reg√°lame un cafecito" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script>
  </div>
```