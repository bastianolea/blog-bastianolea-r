---
title: 'Tutorial: web scraping de sitios web con R usando {rvest}'
author: Bastián Olea Herrera
date: '2024-12-27'
draft: false
format: 
  hugo-md:
    output-file: "index"
    output-ext:  "md"
slug: []
categories:
  - tutoriales
tags:
  - web scraping
  - datos
execute: 
  cache: true
editor_options: 
  chunk_output_type: console
---


Se denomina web scraping a un conjunto de técnicas usadas para **obtener datos desde páginas web**. Esto significa poder transformar la información que vemos en distintos sitios de internet en datos que podamos utilizar. 

Se usa el web scraping cuando un sitio web presenta información, cifras, datos, números, o cualquier otro elemento que nos pueda servir, pero sin facilitar acceso a los datos, como sería un enlace de descarga, una API para obtener los datos, o alguna forma de exportar la información. En estos casos tenemos que recurrir al scraping para transformar lo que vemos en la web en datos analizables.

En este tutorial aprenderemos a hacer web scraping en R para **extraer cualquier información que veamos en un sitio web**, y traerla a nuestro entorno de R para poder procesarla como deseemos.

## Web scraping con {rvest}

La primera opción a la hora de hacer web scraping de sitios web con R es [el paquete {rvest}](https://rvest.tidyverse.org) (_harvest,_ o _cosechar_ en español). Este paquete, parte del [Tidyverse](https://www.tidyverse.org/), suele ser la opción más sencilla, más popular y mejor documentada para extraer datos desde sitios web estáticos en R.

```{r}
# install.packages("rvest")
library(rvest)
```


Existen otros paquetes para el web scraping, como [{RSelenium}](https://docs.ropensci.org/RSelenium/index.html), que se caracteriza por permitirnos obtener datos desde sitios web dinámicos, pero a su vez es menos intuitivo y más difícil de configurar, lo que hace que sea necesario explicarlo en un tutorial aparte.


### Extraer tablas desde un sitio web

Como primer paso, extraeremos los datos de una tabla alojada en internet. Cuando las tablas están formateadas apropiadamente, como las tablas que podemos encontrar en Wikipedia, extraerlas resulta muy sencillo.

El primer paso es definir la dirección del sitio que queremos scrapear, y luego usamos dos funciones para obtener el código de fuente de este sitio: la función `session()`, que nos permite conectarnos al sitio web creando una _sesión_, y luego la función `read_html()`, que descargará el código del sitio y nos permitirá extraer información desde el mismo.

```{r tabla_1}
url <- "https://es.wikipedia.org/wiki/Chile"

sitio_chile <- session(url) |> 
  read_html()

sitio_chile
```

El objeto resultante es un documento HTML del cual podremos extraer los elementos del sitio que nos interesen.

Intentemos extraer la tabla del sitio que posee datos sobre población indígena en el país:

![](scraping_1.jpeg)

Para extraer las tablas de este sitio, usamos la función `html_table()`. Las funciones que extraen elementos de los sitios empiezan con `html`.

```{r tabla_2}
tablas <- sitio_chile |> 
  html_table()
```

El resultado de esta extracción va a ser una lista. En R, una lista es un tipo de objeto que dentro de sí puede contener múltiples elementos de distinto tipo, forma, o tamaño. En este caso, esta lista contiene todas las tablas que habían en la página. Entonces, tenemos que elegir la tabla que nos interesa analizar. 

En este caso, nos interesa la octava tabla, que contiene información sobre pueblos indígenas en Chile, por lo que la extraemos de la lista:

```{r tabla_3}
# extraer tabla de la lista
tabla_1 <- tablas[[8]]

tabla_1

# limpiar datos de la tabla
library(dplyr)

# renombrar columnas
tabla_1a <- tabla_1 |> 
  janitor::row_to_names(1) |> 
  janitor::clean_names()

# convertir variables a numéricas
tabla_1b <- tabla_1a |> 
  # remover espacios
  mutate(poblacion = stringr::str_remove_all(poblacion, "\\s+")) |>  
  # reemplazar comas por puntos
  mutate(percent = stringr::str_replace(percent, ",", ".")) |> 
  # convertir a numéricas
  mutate(across(c(poblacion, percent), as.numeric))

tabla_1b
```

¡Listo! Extrajimos datos desde una tabla de Wikipedia, y ahora podemos usarlos para lo que necesitemos.


### Extraer datos de un sitio usando etiquetas

Para scrapear elementos puntuales de un sitio, necesitamos identificarlos, y así extraer solamente lo que nos interesa. Cada elemento en un sitio web, ya sea texto, tablas o imágenes, se crea a partir de [etiquetas HTML](https://developer.mozilla.org/es/docs/Web/HTML/Element). Si identificamos las etiquetas HTML de los datos que necesitamos, podemos usarlas para extraerlos.

Probemos con otra página de Wikipedia: 

![](scraping_2.jpeg)

```{r sitio_1}
url <- "https://es.wikipedia.org/wiki/Mapache"

sitio_mapache <- session(url) |> 
  read_html()
```

Para extraer elementos de un sitio, usamos la función `html_elements()`, cuyo argumento será el identificador del elemento que queremos extraer. Para extraer el título de la página, usualmente podemos usar la etiqueta HTML `h1`, que se usa para títulos de mayor jerarquía (existen etiquetas de títulos `h1`, `h2`... hasta `h6`).

```{r sitio_2}
sitio_mapache |> 
  html_elements("h1")
```

Obtenemos un elemento que empieza con `<h1...`, lo que significa que extrajimos un elemento en el sitio con dicha etiqueta `h1`. Para convertir este elemento a texto, usamos la función `html_text()`:

```{r sitio_3}
sitio_mapache |> 
  html_elements("h1") |> 
  html_text()
```

Del mismo modo, podemos extraer otros elementos del sitio, tales como los subtítulos de etiqueta `h2`:

```{r sitio_4}
sitio_mapache |> 
  html_elements("h2") |> 
  html_text()
```

Si deseamos extraer todo el texto del sitio, apuntamos a la etiqueta de párrafo, que es `p`:

```{r sitio_5}
texto <- sitio_mapache |> 
  html_elements("p") |> 
  html_text()

texto[3:5]
```

El objeto resultante es un vector de texto, separado en párrafos.


### Extraer datos de un sitio usando clases

Hagamos otro ejemplo sobre un sitio menos estructurado que Wikipedia: queremos obtener una lista de animales endémicos de Chile desde el sitio web [animalia.bio](https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile). 

En este sitio, cada títuar de los párrafos contiene el nombre de un animal endémico. Por alguna razón, este sitio impide que le hagamos scraping si nos conectamos con la función `session()`[^1], pero también es posible extraer el código directamente del sitio, sin crear una sesión. Cuándo se hace web scraping es posible encontrarse con problemas de este tipo, como sitios que evitan entregar su información, o que lo hacen un poco más complicado de lo normal.

[^1]: Lo más probable sea que este sitio identifique que estamos conectándonos desde un paquete de R que hace web scraping, y por lo tanto cancela la conexión entregando un error 403 (prohibido). Esto se debe a que cualquier interfaz que se conecte una página web entrega un _user agent_ que los identifica, y muchos sitios usan esta información para prohibir el acceso o mostrar distintos elementos dependiendo del navegador que uses.

```{r sitio_6}
url <- "https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile"

# extraer código de fuente del sitio sin abrir una sesión
sitio_animales <- read_html(url)

sitio_animales
```

Con este sitio web no nos resulta extraer la etiqueta `h2`, `h3` o `h4` que se usan usualmente para los titulares en sus distintos niveles, sencillamente porque este sitio web no usa esas etiquetas para crear sus titulares. 

```{r sitio_7}
sitio_animales |> 
  html_elements("h2")
```

En estos casos, tenemos que entrar al sitio web e identificar manualmente cómo se distinguen entre el resto del código los elementos que queremos extraer. 

Accedemos al sitio web con un navegador web cualquiera, y entramos al inspector web de nuestro navegador. Podemos hacer clic derecho en algún elemento del sitio, y elegir la opción _Inspeccionar_ para abrir el inspector:

![](scraping_3.jpeg)

Se abrirá un **inspector web** donde veremos el código de fuente del sitio al lado del sitio mismo. Si movemos nuestro cursor sobre las líneas del código, se destacarán los elementos del sitio web que corresponden a cada línea, o viceversa. De esta forma, podemos encontrar exactamente cuál línea de código se corresponde con el elemento que queremos extraer.

![](scraping_4.jpeg)

En este caso, la línea de código de los titulares de cada animal es a una etiqueta `span`, que corresponde a un contenedor de texto. Por sí sola, esta etiqueta no identifica de manera única a los datos que queremos extraer, por lo tanto, tenemos que encontrar otro identificador dentro de esta etiqueta que nos sirva. 

En la mayoría de los casos, los elementos de un sitio web que comparten un mismo estilo gráfico (tipografía, tamaño de texto, color, etc.) comparten también una **clase CSS**. Una clase CSS es una forma de definir la apariencia de una parte de un sitio web, como un titular, un párrafo de texto, o un botón, definiendo su apariencia en una hoja de estilos CSS a partir del nombre de la clase, para luego aplicar esta misma clase a múltiples elementos del sitio. Por lo tanto, **identificar la clase de un elemento web usualmente nos permite extraer múltiples elementos de un sitio que comparten una misma jerarquía o apariencia.** 

En el caso de este sitio, y como vemos en el código del inspector web de nuestro navegador, todos los títulos de los animales del sitio poseen la clase `collection-animal-title`. Entonces, podemos usar esa clase para extraer los elementos usando la función `html_elements()`. Pero hay que tener en consideración que las clases CSS se escriben anteponiéndoles un punto, por lo que usamos `".collection-animal-title"`:

```{r sitio_8}
animales <- sitio_animales |> 
  html_elements(".collection-animal-title") |> 
  html_text()

animales[1:10]
```

Al extraer los elementos que comparten una misma clase, `{rvest}` nos entrega un vector que contiene todos elementos del sitio que usan en esa clase; en este caso, los títulos de los animales.

Si seguimos viendo el código de fuente del sitio, notamos que también hay otras clases que describen los elementos asociados a cada animal: la clase `collection-desc` que se aplica a los párrafos, y la clase `animal-link` que contiene el enlace a cada animal.

Para extraer los textos, apuntamos a la clase `.collection-desc`, y usamos `html_text2()`, que además de convertir a texto, también ayuda removiendo caracteres en blanco como espacios o saltos de línea:

```{r  sitio_9}
textos <- sitio_animales |> 
  html_elements(".collection-desc") |> 
  html_text2()

textos[1:5]
```

Para extraer los enlaces, el proceso es levemente distinto, porque si extraemos el texto de los enlaces desde la clase `.animal-link`, solamente obtendremos el texto, no el enlace en sí mismo:

```{r sitio_10a}
sitio_animales |> 
  html_elements(".animal-link") |> 
  html_text()
```

Notamos que la etiqueta HTML que se usa para crear los enlaces es la etiqueta `a`, y dentro de esta etiqueta, además de la clase tenemos otros **atributos**, tales como el atributo `href` (_hypertext reference_), que contiene la dirección a la que apunta el enlace:

```{r sitio_10b}
sitio_animales |> 
  html_elements(".animal-link")
```

Para extraer este atributo de los elementos de etiqueta `a`, usamos la función `html_attr()`, especificando el atributo que queremos extraer:

```{r sitio_10c}
enlaces <- sitio_animales |> 
  html_elements(".animal-link") |> 
  html_attr("href")

enlaces[1:10]
```

Una vez que hayamos extraído los elementos del sitio que necesitamos, y considerando que los elementos extraídos tienen todos el mismo largo (dado que cada animal del sitio tenía exactamente un título, un párrafo y un enlace), podemos unir todos los resultados en una tabla para **construir un dataframe a partir de los datos scrapeados.**

```{r}
tabla_animales <- tibble(animales,
                         textos, 
                         enlaces)

tabla_animales
```

Creamos una tabla con los resultados de los tres datos que scrapeamos! 

En general, ésta es la lógica básica del web scraping. Se debe identificar el sitio web que necesitas, luego explorar el código de fuente del sitio para entender cuáles son las clases de los elementos que quieres extraer, para finalmente extraerlos y procesarlos, de ser necesario. En muchos casos, lo que se hace es obtener una cierta cantidad de enlaces que contienen información que está estructurada de una manera homogénea, y luego se ejecuta el proceso de web scraping a cada uno de los enlaces a través de un loop o una iteración.

----

Si te sirvió este tutorial, por favor considera hacerme una pequeña donación para poder tomarme un cafecito mientras escribo el siguiente tutorial 🥺

```{=html}
<div style = "height: 18px;">
</div>
<div>
  <div style="display: flex;
  justify-content: center;
  align-items: center;">
    <script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="bastimapache" data-color="#FFDD00" data-emoji="☕"  data-font="Cookie" data-text="Regálame un cafecito" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff" ></script>
  </div>
```