---
title: Visualizando texto como nubes de palabras en R
author: Basti치n Olea Herrera
date: '2025-07-05'
draft: false
format:
  hugo-md:
    output-file: "index"
    output-ext: "md"
slug: []
categories: []
tags:
  - visualizaci칩n de datos
  - gr치ficos
  - ggplot2
  - an치lisis de texto
---

Una de las formas m치s intuitivas de visualizar datos de texto son las nubes de palabras. En las nubes de palabras seleccionamos un subconjunto de las palabras del texto que queremos analizar y las distribuimos en un gr치fico, donde las palabras que aparecen m치s frecuentemente aparecen m치s grandes, y usualmente al centro. Sirven para ver r치pidamente los conceptos clave de un documento o un corpus de documentos.

En este post veremos dos formas de crear nubes de palabras con R: con `{wordcloud2}` y con `{ggplot2}`. Para empezar, necesitamos una base de datos que tenga informaci칩n de texto; por ejemplo, una base donde cada fila contenga una respuesta abierta de una encuesta, una rese침a de un producto, un p치rrafo de un texto, un cap칤tulo de un libro, o un libro completo. 

Con el siguiente c칩digo descargaremos una base de datos con 10.000 documentos de texto, en este caso se trata de noticias de la prensa chilena. Los datos son obtenidos de [este repositorio de obtenci칩n automatizada de textos de noticias de prensa escrita chilena.](https://github.com/bastianolea/prensa_chile)

```{r}
#| message: false

library(dplyr)

# direcci칩n web donde se encuentran los datos
url_datos <- "https://raw.githubusercontent.com/bastianolea/prensa_chile/refs/heads/main/datos/prensa_datos_muestra.csv"

# lectura de los datos ubicados en internet
noticias <- readr::read_csv2(url_datos)

noticias |> select(titulo, cuerpo, fecha)
```

En este conjunto de datos, los textos completos de los documentos vienen en la columna `cuerpo`:

```{r}
noticias |> select(cuerpo)
```

Usualmente los datos de texto van a venir de esta forma, con **todo el texto de cada documento dentro de una celda.** Para poder analizar los datos en este formato tenemos que _tokenizar_ los textos: transformar la estructura de los datos para pasar desde filas que contienen documentos, p치rrafos u oraciones, a **filas que contienen palabras individuales**, una palabra por observaci칩n.

Usaremos la funci칩n `unnest_tokens` de [`{tidytext}`](https://www.tidytextmining.com/tidytext) para tokenizar y limpiar[^1] el texto:

[^1]: La limpieza del texto es un paso clave en este tipo de an치lisis, que para simplificar esta gu칤a, omitiremos. Es muy importante procesar los textos para poder eliminar palabras vac칤as, palabras mal escritas, remover s칤mbolos, remover palabras irrelevantes, e incluso lematizar palabras para agrupar diferentes conjugaciones de una misma palabra en una sola. 

```{r}
library(tidytext)

palabras <- noticias |> 
  # tokenizar columna que contiene los documentos
  unnest_tokens(input = cuerpo, drop = FALSE,
                output = palabra, 
                token = "words",
                # limpieza m칤nima de texto
                to_lower = TRUE,
                strip_punct = TRUE,
                strip_numeric = TRUE) |> 
  # eliminar stopwords o palabras vac칤as
  filter(!palabra %in% stopwords::stopwords("spanish"))

palabras |> select(palabra, cuerpo)
```

Luego de tokenizaci칩n, vemos que la columna que contiene el texto completo de los documentos est치 acompa침ada de una nueva variable que contiene **cada palabra por separado.** De esta forma puedes continuar analizando el documento con palabras individuales, pero teniendo como referencia al texto completo donde aparece cada palabra si es que lo necesitas.

A partir de la variable de palabras individuales, vamos a hacer un **conteo de las palabras m치s frecuentes**:

```{r}
palabras_conteo <- palabras |> 
  count(palabra, sort = TRUE)

palabras_conteo
```

R치pidamente podemos ver dos problemas: tenemos demasiadas palabras individuales (m치s de 80 mil), y tenemos palabras muy cortas, como la palabra `si`, as칤 que haremos una peque침a limpieza:

```{r}
palabras_conteo <- palabras_conteo |> 
  # largo m칤nimo de palabras
  filter(nchar(palabra) >= 3) |> 
  # s칩lo las palabras m치s frecuentes
  slice_max(n, n = 2000)
```

Tambi칠n podemos realizar una **b칰squeda** dentro de las palabras y contar cu치ntas veces aparece cada una:
```{r}
palabras_conteo |> 
  filter(palabra %in% c("guerra", "paz"))
```

----


## Nube de palabras con `{wordcloud2}`
[`{wordcloud2}`](https://github.com/Lchiffon/wordcloud2) es un paquete muy sencillo de usar que permite visualizar datos de nubes de palabras con muy poca configuraci칩n.

Si no tienes el paquete instalado, puedes instalarlo con el siguiente c칩digo:

```{r}
#| eval: false
devtools::install_github("lchiffon/wordcloud2")
```

Para crear una nube de palabras con `{wordcloud2}` solo necesitas que la primera columna del dataframe contenga las palabras, y la segunda contenga el conteo de frecuencia de las palabras:

```{r nube_1}
#| echo: false
#| output: false
library(wordcloud2)

nube_1 <- palabras_conteo |>
  # s칩lo palabras que aparezcan n veces
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74")

htmlwidgets::saveWidget(widget = nube_1,
                        file = here::here("static/nube_1.html"),
                        selfcontained = TRUE)
```

```{r}
#| eval: false

library(wordcloud2)

palabras_conteo |>
  # s칩lo palabras que aparezcan n veces
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74")
```

```{=html}
<iframe src="/nube_1.html" width="100%" height="500px" style="border:none;"></iframe>
```

Usamos algunas de las opciones de personalizaci칩n de `wordcloud2()` para obtener el resultado que necesitamos. Personalmente no me gusta mucho que las palabras aparezcan tan en 치ngulo, y prefiero que las palabras sean menos grandes para que se vea una mayor cantidad.

```{r nube_2}
#| echo: false
#| output: false

nube_2 <- palabras_conteo |>
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74",
             # personalizaci칩n
             rotateRatio = 0.1, # rotaci칩n m치xima
             gridSize = 8, # espaciado entre cada palabra
             size = 0.5, # tama침o del texto en general
             minSize = 11) # tama침o m칤nimo de las letras

htmlwidgets::saveWidget(widget = nube_2,
                        file = here::here("static/nube_2.html"),
                        selfcontained = TRUE)
```

```{r}
#| eval: false
palabras_conteo |>
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74",
             # personalizaci칩n
             rotateRatio = 0.1, # rotaci칩n m치xima
             gridSize = 8, # espaciado entre cada palabra
             size = 0.5, # tama침o del texto en general
             minSize = 11) # tama침o m칤nimo de las letras
```


```{=html}
<iframe src="/nube_2.html" width="100%" height="500px" style="border:none;"></iframe>
</iframe>
```


----

## Nube de palabras con `{ggplot2}` y `{ggwordcloud}`

Otra forma de hacer nubes de palabras es agregando una geometr칤a personalizada a `{ggplot2}`: `geom_text_wordcloud()` del paquete [`{ggwordcloud}`](https://lepennec.github.io/ggwordcloud/)

Instalar el paquete:
```{r}
#| eval: false
devtools::install_github("lepennec/ggwordcloud")
```

Configuramos un tema general para los gr치ficos, para que se vean bonitos aqu칤 游봃

```{r}
library(ggplot2)
library(ggwordcloud)

# definir el tema para todos los gr치ficos
theme_set(
  theme_void() +
  theme(plot.background = element_rect(fill = "#EAD1FA", linewidth = 0))
)
```

Para crear la nube de palabras con `{ggplot2}`, solamente tenemos que especificar en la est칠tica `aes()` la variable que contiene las palabras (`palabra`) y la variable que controla el tama침o de las mismas (`n`). Luego simplemente indicar que los datos van a visualizarse por medio de la geometr칤a `geom_text_wordcloud()`, que se encargar치 de distribuir los textos en el 치rea del gr치fico.

```{r}
palabras_conteo |> 
  filter(n > 2000) |> 
  ggplot() +
  aes(label = palabra, size = n) +
  geom_text_wordcloud(shape = "circle", 
                      color = "#543A74") +
  # definir rango de tama침os de las palabras
  scale_size_continuous(range = c(3, 20))
```
El beneficio de usar esta estrategia es que te entrega toda la flexibilidad de visualizar datos con `{ggplot2}`. Por ejemplo, podemos agregar un mapeo de la transparencia y el color para crear una nube donde las palabras menos frecuentes sean m치s transparentes y donde ciertas palabras claves tengan un color distinto:

```{r}
palabras_conteo |> 
  filter(n > 2000) |> 
  mutate(clave = ifelse(palabra %in% c("gobierno", "presidente", "boric", "ministerio"),
                         "clave", "otras")) |> 
  ggplot() +
  aes(label = palabra, size = n, 
      alpha = n, 
      color = clave) +
  geom_text_wordcloud(shape = "circle", 
                      grid_size = 6) +
  # definir rango de tama침o de palabras
  scale_size_continuous(range = c(3, 15)) +
  # especificar colores de las palabras clave
  scale_color_manual(values = c("clave" = "#D93E98", "otras" = "#543A74")) +
  # definir el rango de la transparencia de palabras
  scale_alpha_continuous(range = c(0.4, 1))
```
Tambi칠n podemos crear una variable que destaques ciertas palabras espec칤ficas dentro de la nube:

```{r}
palabras_conteo |> 
  filter(n > 1500) |> 
  # crear variable que destaque palabras espec칤ficas
  mutate(clave = ifelse(palabra %in% c("gobierno", "chile",
                                       "presidente", "boric", 
                                       "ministerio", "ministra", "interior",
                                       "partido",
                                       "p칰blico"),
                         "clave", "otras")) |> 
  ggplot() +
  aes(label = palabra, size = n, 
      # mapear transparencia a la variable con palabras clave
      alpha = clave) +
  geom_text_wordcloud(shape = "circle",
                      color = "#543A74",
                      rm_outside = TRUE) +
  # especificar rango de tama침os
  scale_size_continuous(range = c(3, 15)) +
  # especificar nivel de transparencia de las palabras clave y de las dem치s
  scale_alpha_manual(values = c("clave" = 1, "otras" = 0.3))
```

Con esta aproximaci칩n podemos crear visualizaciones m치s personalizadas y complejas a partir de datos de texto, sobre todo si tenemos variables adicionales que nos digan m치s informaci칩n sobre las palabras que queremos graficar, c칩mo puede ser alguna otro criterio de prevalencia de palabras en los documentos, alguna clasificaci칩n de las palabras en t칩picos o temas, una agrupaci칩n de los documentos de donde provienen las palabras, etc.

----

{{< cursos >}}

{{< cafecito >}}