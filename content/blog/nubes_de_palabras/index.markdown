---
title: Visualizando texto como nubes de palabras en R
author: Basti√°n Olea Herrera
date: '2025-07-05'
draft: false
format:
  hugo-md:
    output-file: "index"
    output-ext: "md"
slug: []
categories: []
tags:
  - visualizaci√≥n de datos
  - gr√°ficos
  - ggplot2
  - an√°lisis de texto
---

Una de las formas m√°s intuitivas de visualizar datos de texto son las nubes de palabras. En las nubes de palabras seleccionamos un subconjunto de las palabras del texto que queremos analizar y las distribuimos en un gr√°fico, donde las palabras que aparecen m√°s frecuentemente aparecen m√°s grandes, y usualmente al centro. Sirven para ver r√°pidamente los conceptos clave de un documento o un corpus de documentos.

En este post veremos dos formas de crear nubes de palabras con R: con `{wordcloud2}` y con `{ggplot2}`. Para empezar, necesitamos una base de datos que tenga informaci√≥n de texto; por ejemplo, una base donde cada fila contenga una respuesta abierta de una encuesta, una rese√±a de un producto, un p√°rrafo de un texto, un cap√≠tulo de un libro, o un libro completo.

Con el siguiente c√≥digo descargaremos una base de datos con 10.000 documentos de texto, en este caso se trata de noticias de la prensa chilena. Los datos son obtenidos de [este repositorio de obtenci√≥n automatizada de textos de noticias de prensa escrita chilena.](https://github.com/bastianolea/prensa_chile)

``` r
library(dplyr)

# direcci√≥n web donde se encuentran los datos
url_datos <- "https://raw.githubusercontent.com/bastianolea/prensa_chile/refs/heads/main/datos/prensa_datos_muestra.csv"

# lectura de los datos ubicados en internet
noticias <- readr::read_csv2(url_datos)

noticias |> select(titulo, cuerpo, fecha)
```

    ## # A tibble: 10,000 √ó 3
    ##    titulo                                                      cuerpo fecha     
    ##    <chr>                                                       <chr>  <date>    
    ##  1 "Hombre de 66 a√±os fue asesinado en plena v√≠a p√∫blica en M‚Ä¶ "El h‚Ä¶ 2024-07-21
    ##  2 "Revive el cuarto cap√≠tulo de Indecisos: Candidatos a alca‚Ä¶ "Este‚Ä¶ 2024-09-03
    ##  3 "Menos personas circulando y miedo de los residentes: As√≠ ‚Ä¶ "Poca‚Ä¶ 2024-04-08
    ##  4 "CEO de Walmart Chile sostiene que si la empresa no da ‚Äúse‚Ä¶ "Hace‚Ä¶ 2024-12-22
    ##  5 "Pdte. Boric entreg√≥ mensaje por la muerte de Pi√±era y rec‚Ä¶ "El p‚Ä¶ 2024-02-06
    ##  6 "Encuentran dos cad√°veres en un canal de regad√≠o en Pirque" "Pers‚Ä¶ 2024-07-13
    ##  7 "Magisterio denuncia \"abandono de la educaci√≥n p√∫blica\" ‚Ä¶ "Carl‚Ä¶ 2024-02-21
    ##  8 "‚ÄúEl d√≠a que la democracia triunf√≥ sobre la dictadura‚Äù: Mi‚Ä¶ "Los ‚Ä¶ 2024-10-05
    ##  9 "Accidente de tr√°nsito deja un fallecido y tres heridos en‚Ä¶ "En h‚Ä¶ 2024-12-22
    ## 10 "Dos personas resultan baleadas tras intentar evitar asalt‚Ä¶ "Aton‚Ä¶ 2024-11-02
    ## # ‚Ñπ 9,990 more rows

En este conjunto de datos, los textos completos de los documentos vienen en la columna `cuerpo`:

``` r
noticias |> select(cuerpo)
```

    ## # A tibble: 10,000 √ó 1
    ##    cuerpo                                                                       
    ##    <chr>                                                                        
    ##  1 "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue abordada p‚Ä¶
    ##  2 "Este martes 3 de septiembre fue el cuarto cap√≠tulo de la nueva temporada de‚Ä¶
    ##  3 "Pocas personas circulan en la toma \"Nuevo Amanecer\" de Cerrillos, luego d‚Ä¶
    ##  4 "Hace unos d√≠as, Walmart Chile anunci√≥ un plan de inversi√≥n en Chile de US$1‚Ä¶
    ##  5 "El presidente Gabriel Boric lament√≥ el fallecimiento del expresidente Sebas‚Ä¶
    ##  6 "Personal del Grupo de Operaciones Policiales Especiales (GOPE) de Carabiner‚Ä¶
    ##  7 "Carlos Rodr√≠guez, presidente regional de los Profesores, asegur√≥ que el Min‚Ä¶
    ##  8 "Los ministros de Educaci√≥n, Nicol√°s Cataldo, y del Interior, Carolina Toh√°,‚Ä¶
    ##  9 "En horas de la ma√±ana de este domingo, ocurri√≥ un lamentable accidente en l‚Ä¶
    ## 10 "Aton / Imagen Referencial Dos personas resultaron heridas a bala en un inte‚Ä¶
    ## # ‚Ñπ 9,990 more rows

Usualmente los datos de texto van a venir de esta forma, con **todo el texto de cada documento dentro de una celda.** Para poder analizar los datos en este formato tenemos que *tokenizar* los textos: transformar la estructura de los datos para pasar desde filas que contienen documentos, p√°rrafos u oraciones, a **filas que contienen palabras individuales**, una palabra por observaci√≥n.

Usaremos la funci√≥n `unnest_tokens` de [`{tidytext}`](https://www.tidytextmining.com/tidytext) para tokenizar y limpiar[^1] el texto:

``` r
library(tidytext)

palabras <- noticias |> 
  # tokenizar columna que contiene los documentos
  unnest_tokens(input = cuerpo, drop = FALSE,
                output = palabra, 
                token = "words",
                # limpieza m√≠nima de texto
                to_lower = TRUE,
                strip_punct = TRUE,
                strip_numeric = TRUE) |> 
  # eliminar stopwords o palabras vac√≠as
  filter(!palabra %in% stopwords::stopwords("spanish"))

palabras |> select(palabra, cuerpo)
```

    ## # A tibble: 1,817,901 √ó 2
    ##    palabra   cuerpo                                                             
    ##    <chr>     <chr>                                                              
    ##  1 hecho     "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  2 ocurri√≥   "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  3 horas     "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  4 madrugada "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  5 v√≠ctima   "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  6 abordada  "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  7 grupo     "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  8 sujetos   "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ##  9 atac√≥     "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ## 10 arma      "El hecho ocurri√≥ en horas de la madrugada, cuando la v√≠ctima fue ‚Ä¶
    ## # ‚Ñπ 1,817,891 more rows

Luego de tokenizaci√≥n, vemos que la columna que contiene el texto completo de los documentos est√° acompa√±ada de una nueva variable que contiene **cada palabra por separado.** De esta forma puedes continuar analizando el documento con palabras individuales, pero teniendo como referencia al texto completo donde aparece cada palabra si es que lo necesitas.

A partir de la variable de palabras individuales, vamos a hacer un **conteo de las palabras m√°s frecuentes**:

``` r
palabras_conteo <- palabras |> 
  count(palabra, sort = TRUE)

palabras_conteo
```

    ## # A tibble: 74,911 √ó 2
    ##    palabra      n
    ##    <chr>    <int>
    ##  1 chile     7208
    ##  2 a√±os      5457
    ##  3 seg√∫n     5071
    ##  4 parte     4952
    ##  5 personas  4921
    ##  6 si        4912
    ##  7 regi√≥n    4830
    ##  8 ser       4656
    ##  9 pa√≠s      4628
    ## 10 gobierno  4599
    ## # ‚Ñπ 74,901 more rows

R√°pidamente podemos ver dos problemas: tenemos demasiadas palabras individuales (m√°s de 80 mil), y tenemos palabras muy cortas, como la palabra `si`, as√≠ que haremos una peque√±a limpieza:

``` r
palabras_conteo <- palabras_conteo |> 
  # largo m√≠nimo de palabras
  filter(nchar(palabra) >= 3) |> 
  # s√≥lo las palabras m√°s frecuentes
  slice_max(n, n = 2000)
```

Tambi√©n podemos realizar una **b√∫squeda** dentro de las palabras y contar cu√°ntas veces aparece cada una:

``` r
palabras_conteo |> 
  filter(palabra %in% c("guerra", "paz"))
```

    ## # A tibble: 2 √ó 2
    ##   palabra     n
    ##   <chr>   <int>
    ## 1 guerra    398
    ## 2 paz       267

## Nube de palabras con `{wordcloud2}`

[`{wordcloud2}`](https://github.com/Lchiffon/wordcloud2) es un paquete muy sencillo de usar que permite visualizar datos de nubes de palabras con muy poca configuraci√≥n.

Si no tienes el paquete instalado, puedes instalarlo con el siguiente c√≥digo:

``` r
devtools::install_github("lchiffon/wordcloud2")
```

Para crear una nube de palabras con `{wordcloud2}` solo necesitas que la primera columna del dataframe contenga las palabras, y la segunda contenga el conteo de frecuencia de las palabras:

``` r
library(wordcloud2)

palabras_conteo |>
  # s√≥lo palabras que aparezcan n veces
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74")
```

<iframe src="/nube_1.html" width="100%" height="500px" style="border:none;"></iframe>

Usamos algunas de las opciones de personalizaci√≥n de `wordcloud2()` para obtener el resultado que necesitamos. Personalmente no me gusta mucho que las palabras aparezcan tan en √°ngulo, y prefiero que las palabras sean menos grandes para que se vea una mayor cantidad.

``` r
palabras_conteo |>
  filter(n > 1000) |> 
  wordcloud2(backgroundColor = "#EAD1FA",
             color = "#543A74",
             # personalizaci√≥n
             rotateRatio = 0.1, # rotaci√≥n m√°xima
             gridSize = 8, # espaciado entre cada palabra
             size = 0.5, # tama√±o del texto en general
             minSize = 11) # tama√±o m√≠nimo de las letras
```

<iframe src="/nube_2.html" width="100%" height="500px" style="border:none;"></iframe>
</iframe>

------------------------------------------------------------------------

## Nube de palabras con `{ggplot2}` y `{ggwordcloud}`

Otra forma de hacer nubes de palabras es agregando una geometr√≠a personalizada a `{ggplot2}`: `geom_text_wordcloud()` del paquete [`{ggwordcloud}`](https://lepennec.github.io/ggwordcloud/)

Instalar el paquete:

``` r
devtools::install_github("lepennec/ggwordcloud")
```

Configuramos un tema general para los gr√°ficos, para que se vean bonitos aqu√≠ ü•∞

``` r
library(ggplot2)
library(ggwordcloud)

# definir el tema para todos los gr√°ficos
theme_set(
  theme_void() +
  theme(plot.background = element_rect(fill = "#EAD1FA", linewidth = 0))
)
```

Para crear la nube de palabras con `{ggplot2}`, solamente tenemos que especificar en la est√©tica `aes()` la variable que contiene las palabras (`palabra`) y la variable que controla el tama√±o de las mismas (`n`). Luego simplemente indicar que los datos van a visualizarse por medio de la geometr√≠a `geom_text_wordcloud()`, que se encargar√° de distribuir los textos en el √°rea del gr√°fico.

``` r
palabras_conteo |> 
  filter(n > 2000) |> 
  ggplot() +
  aes(label = palabra, size = n) +
  geom_text_wordcloud(shape = "circle", 
                      color = "#543A74") +
  # definir rango de tama√±os de las palabras
  scale_size_continuous(range = c(3, 20))
```

    ## Warning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :
    ## Some words could not fit on page. They have been placed at their original
    ## positions.

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" />

El beneficio de usar esta estrategia es que te entrega toda la flexibilidad de visualizar datos con `{ggplot2}`. Por ejemplo, podemos agregar un mapeo de la transparencia y el color para crear una nube donde las palabras menos frecuentes sean m√°s transparentes y donde ciertas palabras claves tengan un color distinto:

``` r
palabras_conteo |> 
  filter(n > 2000) |> 
  mutate(clave = ifelse(palabra %in% c("gobierno", "presidente", "boric", "ministerio"),
                         "clave", "otras")) |> 
  ggplot() +
  aes(label = palabra, size = n, 
      alpha = n, 
      color = clave) +
  geom_text_wordcloud(shape = "circle", 
                      grid_size = 6) +
  # definir rango de tama√±o de palabras
  scale_size_continuous(range = c(3, 15)) +
  # especificar colores de las palabras clave
  scale_color_manual(values = c("clave" = "#D93E98", "otras" = "#543A74")) +
  # definir el rango de la transparencia de palabras
  scale_alpha_continuous(range = c(0.4, 1))
```

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" />

Tambi√©n podemos crear una variable que destaques ciertas palabras espec√≠ficas dentro de la nube:

``` r
palabras_conteo |> 
  filter(n > 1500) |> 
  # crear variable que destaque palabras espec√≠ficas
  mutate(clave = ifelse(palabra %in% c("gobierno", "chile",
                                       "presidente", "boric", 
                                       "ministerio", "ministra", "interior",
                                       "partido",
                                       "p√∫blico"),
                         "clave", "otras")) |> 
  ggplot() +
  aes(label = palabra, size = n, 
      # mapear transparencia a la variable con palabras clave
      alpha = clave) +
  geom_text_wordcloud(shape = "circle",
                      color = "#543A74",
                      rm_outside = TRUE) +
  # especificar rango de tama√±os
  scale_size_continuous(range = c(3, 15)) +
  # especificar nivel de transparencia de las palabras clave y de las dem√°s
  scale_alpha_manual(values = c("clave" = 1, "otras" = 0.3))
```

    ## Warning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :
    ## Some words could not fit on page. They have been removed.

<img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" />

Con esta aproximaci√≥n podemos crear visualizaciones m√°s personalizadas y complejas a partir de datos de texto, sobre todo si tenemos variables adicionales que nos digan m√°s informaci√≥n sobre las palabras que queremos graficar, c√≥mo puede ser alguna otro criterio de prevalencia de palabras en los documentos, alguna clasificaci√≥n de las palabras en t√≥picos o temas, una agrupaci√≥n de los documentos de donde provienen las palabras, etc.

------------------------------------------------------------------------

{{< cafecito >}}

[^1]: La limpieza del texto es un paso clave en este tipo de an√°lisis, que para simplificar esta gu√≠a, omitiremos. Es muy importante procesar los textos para poder eliminar palabras vac√≠as, palabras mal escritas, remover s√≠mbolos, remover palabras irrelevantes, e incluso lematizar palabras para agrupar diferentes conjugaciones de una misma palabra en una sola.
