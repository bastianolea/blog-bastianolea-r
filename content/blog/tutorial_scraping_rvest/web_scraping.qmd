---
title: 'Tutorial: web scraping en R usando {rvest}'
author: Basti치n Olea Herrera
date: '2024-12-27'
draft: false
format: 
  hugo-md:
    output-file: "index"
    output-ext:  "md"
slug: []
categories:
  - tutoriales
tags:
  - web scraping
  - datos
execute: 
  cache: true
editor_options: 
  chunk_output_type: console
---


Se denomina web scraping a un conjunto de t칠cnicas usadas para **obtener datos desde p치ginas web**. Esto significa poder transformar la informaci칩n que vemos en distintos sitios de internet en datos que podamos utilizar. 

Se usa el web scraping cuando un sitio web presenta informaci칩n, cifras, datos, n칰meros, o cualquier otro elemento que nos pueda servir, pero sin facilitar acceso a los datos, como ser칤a un enlace de descarga, una API para obtener los datos, o alguna forma de exportar la informaci칩n. En estos casos tenemos que recurrir al scraping para transformar lo que vemos en la web en datos analizables.

En este tutorial aprenderemos a hacer web scraping en R para **extraer cualquier informaci칩n que veamos en un sitio web**, y traerla a nuestro entorno de R para poder procesarla como deseemos.

## Web scraping con {rvest}

La primera opci칩n a la hora de hacer web scraping de sitios web con R es [el paquete {rvest}](https://rvest.tidyverse.org) (_harvest,_ o _cosechar_ en espa침ol). Este paquete, parte del [Tidyverse](https://www.tidyverse.org/), suele ser la opci칩n m치s sencilla, m치s popular y mejor documentada para extraer datos desde sitios web est치ticos en R.

```{r}
# install.packages("rvest")
library(rvest)
```


Existen otros paquetes para el web scraping, como [{RSelenium}](https://docs.ropensci.org/RSelenium/index.html), que se caracteriza por permitirnos obtener datos desde sitios web din치micos, pero a su vez es menos intuitivo y m치s dif칤cil de configurar, lo que hace que sea necesario explicarlo en un tutorial aparte.


### Extraer tablas desde un sitio web

Como primer paso, extraeremos los datos de una tabla alojada en internet. Cuando las tablas est치n formateadas apropiadamente, como las tablas que podemos encontrar en Wikipedia, extraerlas resulta muy sencillo.

El primer paso es definir la direcci칩n del sitio que queremos scrapear, y luego usamos dos funciones para obtener el c칩digo de fuente de este sitio: la funci칩n `session()`, que nos permite conectarnos al sitio web creando una _sesi칩n_, y luego la funci칩n `read_html()`, que descargar치 el c칩digo del sitio y nos permitir치 extraer informaci칩n desde el mismo.

```{r tabla_1}
url <- "https://es.wikipedia.org/wiki/Chile"

sitio_chile <- session(url) |> 
  read_html()

sitio_chile
```

El objeto resultante es un documento HTML del cual podremos extraer los elementos del sitio que nos interesen.

Intentemos extraer la tabla del sitio que posee datos sobre poblaci칩n ind칤gena en el pa칤s:

![](scraping_1.jpeg)

Para extraer las tablas de este sitio, usamos la funci칩n `html_table()`. Las funciones que extraen elementos de los sitios empiezan con `html`.

```{r tabla_2}
tablas <- sitio_chile |> 
  html_table()
```

El resultado de esta extracci칩n va a ser una lista. En R, una lista es un tipo de objeto que dentro de s칤 puede contener m칰ltiples elementos de distinto tipo, forma, o tama침o. En este caso, esta lista contiene todas las tablas que hab칤an en la p치gina. Entonces, tenemos que elegir la tabla que nos interesa analizar. 

En este caso, nos interesa la octava tabla, que contiene informaci칩n sobre pueblos ind칤genas en Chile, por lo que la extraemos de la lista:

```{r tabla_3}
# extraer tabla de la lista
tabla_1 <- tablas[[8]]

tabla_1

# limpiar datos de la tabla
library(dplyr)

# renombrar columnas
tabla_1a <- tabla_1 |> 
  janitor::row_to_names(1) |> 
  janitor::clean_names()

# convertir variables a num칠ricas
tabla_1b <- tabla_1a |> 
  # remover espacios
  mutate(poblacion = stringr::str_remove_all(poblacion, "\\s+")) |>  
  # reemplazar comas por puntos
  mutate(percent = stringr::str_replace(percent, ",", ".")) |> 
  # convertir a num칠ricas
  mutate(across(c(poblacion, percent), as.numeric))

tabla_1b
```

춰Listo! Extrajimos datos desde una tabla de Wikipedia, y ahora podemos usarlos para lo que necesitemos.


### Extraer datos de un sitio usando etiquetas

Para scrapear elementos puntuales de un sitio, necesitamos identificarlos, y as칤 extraer solamente lo que nos interesa. Cada elemento en un sitio web, ya sea texto, tablas o im치genes, se crea a partir de [etiquetas HTML](https://developer.mozilla.org/es/docs/Web/HTML/Element). Si identificamos las etiquetas HTML de los datos que necesitamos, podemos usarlas para extraerlos.

Probemos con otra p치gina de Wikipedia: 

![](scraping_2.jpeg)

```{r sitio_1}
url <- "https://es.wikipedia.org/wiki/Mapache"

sitio_mapache <- session(url) |> 
  read_html()
```

Para extraer elementos de un sitio, usamos la funci칩n `html_elements()`, cuyo argumento ser치 el identificador del elemento que queremos extraer. Para extraer el t칤tulo de la p치gina, usualmente podemos usar la etiqueta HTML `h1`, que se usa para t칤tulos de mayor jerarqu칤a (existen etiquetas de t칤tulos `h1`, `h2`... hasta `h6`).

```{r sitio_2}
sitio_mapache |> 
  html_elements("h1")
```

Obtenemos un elemento que empieza con `<h1...`, lo que significa que extrajimos un elemento en el sitio con dicha etiqueta `h1`. Para convertir este elemento a texto, usamos la funci칩n `html_text()`:

```{r sitio_3}
sitio_mapache |> 
  html_elements("h1") |> 
  html_text()
```

Del mismo modo, podemos extraer otros elementos del sitio, tales como los subt칤tulos de etiqueta `h2`:

```{r sitio_4}
sitio_mapache |> 
  html_elements("h2") |> 
  html_text()
```

Si deseamos extraer todo el texto del sitio, apuntamos a la etiqueta de p치rrafo, que es `p`:

```{r sitio_5}
texto <- sitio_mapache |> 
  html_elements("p") |> 
  html_text()

texto[3:5]
```

El objeto resultante es un vector de texto, separado en p치rrafos.


### Extraer datos de un sitio usando clases

Hagamos otro ejemplo sobre un sitio menos estructurado que Wikipedia: queremos obtener una lista de animales end칠micos de Chile desde el sitio web [animalia.bio](https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile). 

En este sitio, cada t칤tuar de los p치rrafos contiene el nombre de un animal end칠mico. Por alguna raz칩n, este sitio impide que le hagamos scraping si nos conectamos con la funci칩n `session()`[^1], pero tambi칠n es posible extraer el c칩digo directamente del sitio, sin crear una sesi칩n. Cu치ndo se hace web scraping es posible encontrarse con problemas de este tipo, como sitios que evitan entregar su informaci칩n, o que lo hacen un poco m치s complicado de lo normal.

[^1]: Lo m치s probable sea que este sitio identifique que estamos conect치ndonos desde un paquete de R que hace web scraping, y por lo tanto cancela la conexi칩n entregando un error 403 (prohibido). Esto se debe a que cualquier interfaz que se conecte una p치gina web entrega un _user agent_ que los identifica, y muchos sitios usan esta informaci칩n para prohibir el acceso o mostrar distintos elementos dependiendo del navegador que uses.

```{r sitio_6}
url <- "https://animalia.bio/es/endemic-lists/country/endemic-animals-of-chile"

# extraer c칩digo de fuente del sitio sin abrir una sesi칩n
sitio_animales <- read_html(url)

sitio_animales
```

Con este sitio web no nos resulta extraer la etiqueta `h2`, `h3` o `h4` que se usan usualmente para los titulares en sus distintos niveles, sencillamente porque este sitio web no usa esas etiquetas para crear sus titulares. 

```{r sitio_7}
sitio_animales |> 
  html_elements("h2")
```

En estos casos, tenemos que entrar al sitio web e identificar manualmente c칩mo se distinguen entre el resto del c칩digo los elementos que queremos extraer. 

Accedemos al sitio web con un navegador web cualquiera, y entramos al inspector web de nuestro navegador. Podemos hacer clic derecho en alg칰n elemento del sitio, y elegir la opci칩n _Inspeccionar_ para abrir el inspector:

![](scraping_3.jpeg)

Se abrir치 un **inspector web** donde veremos el c칩digo de fuente del sitio al lado del sitio mismo. Si movemos nuestro cursor sobre las l칤neas del c칩digo, se destacar치n los elementos del sitio web que corresponden a cada l칤nea, o viceversa. De esta forma, podemos encontrar exactamente cu치l l칤nea de c칩digo se corresponde con el elemento que queremos extraer.

![](scraping_4-featured.jpeg)

En este caso, la l칤nea de c칩digo de los titulares de cada animal es a una etiqueta `span`, que corresponde a un contenedor de texto. Por s칤 sola, esta etiqueta no identifica de manera 칰nica a los datos que queremos extraer, por lo tanto, tenemos que encontrar otro identificador dentro de esta etiqueta que nos sirva. 

En la mayor칤a de los casos, los elementos de un sitio web que comparten un mismo estilo gr치fico (tipograf칤a, tama침o de texto, color, etc.) comparten tambi칠n una **clase CSS**. Una clase CSS es una forma de definir la apariencia de una parte de un sitio web, como un titular, un p치rrafo de texto, o un bot칩n, definiendo su apariencia en una hoja de estilos CSS a partir del nombre de la clase, para luego aplicar esta misma clase a m칰ltiples elementos del sitio. Por lo tanto, **identificar la clase de un elemento web usualmente nos permite extraer m칰ltiples elementos de un sitio que comparten una misma jerarqu칤a o apariencia.** 

En la siguiente imagen se destacan con l칤neas de colores los elementos del sitio que comparten un estilo y jerarqu칤a, y por ende podemos asumir que comparten una clase en com칰n entre ellos:

![](scraping_5.jpeg)

En el caso de este sitio, y como vemos en el c칩digo del inspector web de nuestro navegador, todos los t칤tulos de los animales del sitio poseen la clase `collection-animal-title`. Entonces, podemos usar esa clase para extraer los elementos usando la funci칩n `html_elements()`. Pero hay que tener en consideraci칩n que las clases CSS se escriben anteponi칠ndoles un punto, por lo que usamos `".collection-animal-title"`:

```{r sitio_8}
animales <- sitio_animales |> 
  html_elements(".collection-animal-title") |> 
  html_text()

animales[1:10]
```

Al extraer los elementos que comparten una misma clase, `{rvest}` nos entrega un vector que contiene todos elementos del sitio que usan en esa clase; en este caso, los t칤tulos de los animales.

Si seguimos viendo el c칩digo de fuente del sitio, notamos que tambi칠n hay otras clases que describen los elementos asociados a cada animal: la clase `collection-desc` que se aplica a los p치rrafos, y la clase `animal-link` que contiene el enlace a cada animal.

Para extraer los textos, apuntamos a la clase `.collection-desc`, y usamos `html_text2()`, que adem치s de convertir a texto, tambi칠n ayuda removiendo caracteres en blanco como espacios o saltos de l칤nea:

```{r  sitio_9}
textos <- sitio_animales |> 
  html_elements(".collection-desc") |> 
  html_text2()

textos[1:5]
```

Para extraer los enlaces, el proceso es levemente distinto, porque si extraemos el texto de los enlaces desde la clase `.animal-link`, solamente obtendremos el texto, no el enlace en s칤 mismo:

```{r sitio_10a}
sitio_animales |> 
  html_elements(".animal-link") |> 
  html_text()
```

Notamos que la etiqueta HTML que se usa para crear los enlaces es la etiqueta `a`, y dentro de esta etiqueta, adem치s de la clase tenemos otros **atributos**, tales como el atributo `href` (_hypertext reference_), que contiene la direcci칩n a la que apunta el enlace:

```{r sitio_10b}
sitio_animales |> 
  html_elements(".animal-link")
```

Para extraer este atributo de los elementos de etiqueta `a`, usamos la funci칩n `html_attr()`, especificando el atributo que queremos extraer:

```{r sitio_10c}
enlaces <- sitio_animales |> 
  html_elements(".animal-link") |> 
  html_attr("href")

enlaces[1:10]
```

Una vez que hayamos extra칤do los elementos del sitio que necesitamos, y considerando que los elementos extra칤dos tienen todos el mismo largo (dado que cada animal del sitio ten칤a exactamente un t칤tulo, un p치rrafo y un enlace), podemos unir todos los resultados en una tabla para **construir un dataframe a partir de los datos scrapeados.**

```{r}
tabla_animales <- tibble(animales,
                         textos, 
                         enlaces)

tabla_animales
```

Creamos una tabla con los resultados de los tres datos que scrapeamos! 

En general, 칠sta es la l칩gica b치sica del web scraping. Se debe identificar el sitio web que necesitas, luego explorar el c칩digo de fuente del sitio para entender cu치les son las clases de los elementos que quieres extraer, para finalmente extraerlos y procesarlos, de ser necesario. En muchos casos, lo que se hace es obtener una cierta cantidad de enlaces que contienen informaci칩n que est치 estructurada de una manera homog칠nea, y luego se ejecuta el proceso de scraping a cada uno de los enlaces a trav칠s de un loop o iteraci칩n, por ejemplo con la funci칩n `purrr::map()`.

El web scraping es una herramienta muy 칰til para acelerar tus procesos y automatizar obtenciones y actualizaciones de datos, y es algo que resulta muy atractivo para personas interesadas en los datos. Pero ojo, que he visto mucha gente que quiere _entrar_ al an치lisis de datos _empezando_ por aprender web scraping. En mi experiencia, si bien el web scraping de por si no es algo complejo, s칤 es una herramienta que requiere de un **dominio de estructuras de datos complejas**, as칤 como de **habilidades avanzadas de limpieza de datos**. Ello debido a que los datos que obtenemos por web scraping pueden venir literalmente en _cualquier_ formato, con infinidad de problemas inesperados, y ser치 tarea de el/la analista resolver esos desaf칤os de interpretaci칩n y limpieza de datos de forma creativa, de manera que los resultados salgan correctos tanto para 1 como para 100 p치ginas de un mismo sitio web.


----

Si te sirvi칩 este tutorial, por favor considera hacerme una peque침a donaci칩n para poder tomarme un cafecito mientras escribo el siguiente tutorial 游봌

{{< cafecito >}}

{{< cursos >}}