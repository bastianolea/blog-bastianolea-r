{
  "hash": "01a429c77e57c19f18f33ec3624e367a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Tutorial: web scraping controlando un navegador web con {RSelenium} en R'\nauthor: Bastián Olea Herrera\ndate: '2025-07-15'\ndraft: false\nfreeze: true\nformat: \n  hugo-md:\n    output-file: \"index\"\n    output-ext:  \"md\"\nslug: []\ncategories:\n  - Tutoriales\ntags:\n  - web scraping\n  - datos\nexecute: \n  eval: false\nexcerpt: Selenium es una herramienta que permite realizar web scraping avanzado por medio del control programático de un navegador web, lo cual abre infinitas posibilidades al momento de automatizar la obtención de datos e información desde sitios web dinámicos y/o complejos. En este tutorial aprenderemos a usar {RSelenium} para programar scripts de R que automaticen el control de un navegador para interactuar con sitios web y así scrapear datos mas difíciles de obtener.\n---\n\n\n\n\nSelenium es un programa para automatizar y controlar navegadores web, lo que lo vuelve en una buena herramienta para realizar **[web scraping](/tags/web-scraping/)**. El [paquete de R `{RSelenium}`](https://docs.ropensci.org/RSelenium/) nos permitirá controlar un navegador por medio de código de R, lo cual abre infinitas posibilidades al momento de automatizar la obtención de datos e información desde sitios web dinámicos y/o complejos. \n\nEn este tutorial aprenderemos a usar `{RSelenium}` para programar scripts de R que automaticen el control de un navegador para interactuar con sitios web y así scrapear datos más difíciles de obtener.\n\n\n\n\n{{< aviso \"Si necesitas aprender lo básico del web scraping, primero [revisa este tutorial de `{rvest}`](/blog/tutorial_scraping_rvest/)\" >}}\n\n\n\n\n\n\nEn otro post aprendimos a [hacer web scraping con `{rvest}`](/blog/tutorial_scraping_rvest/), un paquete muy sencillo de usar para obtener información desde sitios web. Entonces ¿por qué aprender también `{RSelenium}`? La diferencia es que Selenium es capaz de controlar un _navegador web real_ en tu computador, como Chrome o Firefox, lo cual puede marcar la diferencia para extraer datos de sitios que buscan dificultar o impedir el acceso a herramientas automatizadas de web scraping.\n\n\n### Iniciar un cliente\n\nLa primera que necesitamos para hacer web scraping es un navegador web. La función `rsDriver()` se encarga de la descarga e instalación de un navegador para poder usarlo desde R. Elegimos Firefox, y opcionalmente definimos un _puerto_[^1]. \n\n[^1]: El puerto es como la conexión entre tu computador y el navegador, por lo que no pueden haber dos navegadores en un mismo puerto. Si necesitas abrir más de un navegador, o te aparece ocupado el puerto, intenta con otro.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RSelenium)\n\ndriver <- rsDriver(browser = \"firefox\", \n                   port = 4560L, verbose = F,\n                   chromever = NULL, phantomver = NULL)\n```\n:::\n\n\n\n\nÉste código pondrá a descargar el navegador y lo abrirá:\n\n\n\n\n{{< imagen \"rselenium_1.png\" >}}\n\n\n\n\n\n\nAparece una ventana de navegador con la barra superior en rojo, indicando que está siendo controlado por otro programa, en este caso por R.\n\nAhora tenemos que pasarle el control a un objeto de R que representa al navegador remoto:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote <- driver$client\n```\n:::\n\n\n\n\n### Navegar a un sitio web\nDesde el navegador remoto podemos ir ejecutando las acciones que necesitamos. Lo primero es navegar al sitio web que nos interesa\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$navigate(\"https://www.portaltransparencia.cl/PortalPdT/directorio-de-organismos-regulados/?org=MU291\")\n```\n:::\n\n\n\n\nSi nos vamos a ver el navegador, vemos que la página efectivamente se ha cargado.\n\n\n\n\n{{< imagen \"rselenium_2.png\" >}}\n\n\n\n\n\n\nDesde esta página web de Transparencia podemos extraer datos sobre la gestión de organismos públicos, pero para llegar a ellos tenemos que hacer clic en varios enlaces. Usaremos `{RSelenium}` para navegar el sitio y obtener los datos, generando un script que podremos reutilizar para actualizar los datos, obtener datos de manera masiva de este sitio, u obtener los datos del mismo portal pero de otro organismo público, dado que funcionan igual.\n\n\n### Identificar elementos\n\nEn este punto podemos usar el navegador para ir explorando el sitio. Obviamente podríamos ir haciendo clic en el mismo navegador, pero no es la idea. La idea es poder automatizar el proceso por medio de un script que controle las acciones que se hacen en el navegador. Lo que sí podemos hacer es usar las herramientas del navegador para ayudarnos.\n\nSi hacemos clic derecho sobre cualquier elemento del sitio y elegimos _Inspeccionar_, se abrirá el panel de herramientas de desarrollador web.\n\n\n\n\n{{< imagen \"rselenium_3.png\" >}}\n\n\n\n\n\n\nCon este panel podemos inspeccionar el código fuente del sitio web, desde donde podremos extraer texto, enlaces, imágenes, tablas, y más.\n\nEn el panel de herramientas de desarrollador apretamos el _botón de selección_ (destacado con un círculo en la imagen anterior, o `comando + shift + C`) para que cuando pasemos el cursor sobre los elementos del sitio web, se destaque el código fuente correspondiente.\n\n\n\n\n{{< imagen \"rselenium_4.png\" >}}\n\n\n\n\n\n\nCuándo identificamos el elemento que necesitamos, vamos al código correspondiente, hacemos clic derecho, y copiamos el `xpath` o el `selector css`. Ambas son formas de identificar de manera única elementos en un sitio web, pero esta vez copiaremos el `xpath`.\n\n\n\n\n{{< imagen \"rselenium_5.png\" >}}\n\n\n\n\n\n\n### Interactuar con elementos\n\nAhora que tenemos identificado el enlace que queremos apretar por medio de su `xpath`, le decimos al navegador que le haga clic:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$findElement(\"xpath\",\n                   '//*[@id=\"A6428:formInfo:j_idt39:0:datalist:3:j_idt43:4:j_idt47\"]')$\n  clickElement()\n```\n:::\n\n\n\n\nFuncionó! El navegador navegó al enlace que le pedimos.\n\n\n\n\n{{< imagen \"rselenium_6.png\" >}}\n\n\n\n\n\n\nAhora repetimos el proceso para navegar al enlace _Municipal_:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$findElement(\"xpath\", \n                   '//*[@id=\"A6428:formInfo:j_idt76:0:j_idt78\"]')$\n  clickElement()\n```\n:::\n\n\n\n\nAhora el sitio muestra una selección de todos los años que tienen datos. Si volvemos a repetir el proceso, nos damos cuenta de que el `xpath` del año 2025 termina con un `0`, y el del año 2024 termina en `1`, el de 2023 en `2`, y así sucesivamente.\n\n\n\n\n{{< imagen \"rselenium_7.png\" >}}\n\n\n\n\n\n\nEsto significa que podríamos automatizar el acceso a todos los años simplemente creando un loop que vaya desde 0 a 10 (año 2015). Pero por ahora accedamos al segundo año:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$findElement(\"xpath\", \n                   '//*[@id=\"A6428:formInfo:j_idt94:0:j_idt95\"]')$\n  clickElement()\n```\n:::\n\n\n\n\nAhora llegamos a una página con los meses, donde se repita el mismo patrón: todos los meses comparten un `xpath` que termina con el número del cero a la cantidad de meses presentes.\n\n\n\n\n{{< imagen \"rselenium_8.png\" >}}\n\n\n\n\n\n\nSi navegamos alguno de los meses, finalmente llegamos a los datos que estábamos buscando:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$findElement(\"xpath\", \n                   '//*[@id=\"A6428:formInfo:j_idt110:0:j_idt111\"]')$\n  clickElement()\n```\n:::\n\n{{< imagen \"rselenium_9.png\" >}}\n\n\n\n\n\n\n### Obtener el código fuente del sitio\n\nAhora que llegamos a una tabla de datos, podemos pedirle al navegador que nos entregue todo el código de fuente del sitio, para [continuar el scraping con el paquete `{rvest}`](/blog/tutorial_scraping_rvest/):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n\nfuente <- remote$getPageSource()[[1]]\nsitio <- read_html(fuente)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n{{< aviso \"Si necesitas aprender a usar `{rvest}`, [revisa este tutorial primero](/blog/tutorial_scraping_rvest/)\" >}}\n\n\n\n\n\n\nPodemos usar la función `html_elements()` para extraer elementos del sitio por su selector CSS, y convertirlos en texto. Por ejemplo, extraer el texto que está en el título de la tabla:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsitio |> \n  html_elements(\".section-title\") |> \n  html_text2()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"04. Personal y remuneraciones\"\n```\n\n\n:::\n:::\n\n\n\n\n### Extraer tablas\n\nSi es que el sitio web tiene datos en forma de tabla, podemos extraerlo fácilmente con la función `html_table()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabla <- sitio |> \n  html_table()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntabla[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 19\n   Estamento      `Nombre completo`      `Cargo o función` `Grado EUS o jornada`\n   <chr>          <chr>                  <chr>                             <int>\n 1 Auxiliar       ACEVEDO BERTRIN, ANDR… CUADRILLA OPERAC…                    14\n 2 Administrativo AEDO ACEVEDO, ESTEFAN… ADMINISTRATIVA H…                    11\n 3 Administrativo AGUILAR MARTINEZ, ELI… ADMINISTRATIVO D…                    12\n 4 Administrativo AHUMADA COFRE, GERALD… ADMINISTRATIVA V…                    16\n 5 Profesional    ALARCON HERRERA, CARL… PROFESIONAL DE A…                     6\n 6 Administrativo ALARCON NEIRA, CLAUDI… INSPECTOR VIGILA…                    14\n 7 Auxiliar       ALCAINO MARTINEZ, EDU… AUXILIAR ASEO Y …                    14\n 8 Administrativo ALDERETE ALMENDRAS, N… ADMINISTRATIVA V…                    16\n 9 Administrativo ALTAMIRANO ROJAS, ANN… SECRETARIA ADMIN…                    11\n10 Administrativo ALVAREZ FARFAN, CYNTH… INSPECTORA FERIA…                    16\n# ℹ 90 more rows\n# ℹ 15 more variables: `Calificación profesional o formación` <chr>,\n#   Región <chr>, `Asignaciones especiales` <chr>,\n#   `Remuneración bruta mensualizada` <chr>,\n#   `Remuneración líquida mensualizada` <chr>,\n#   `Remuneraciones adicionales` <chr>, `Remuneración Bonos incentivos` <chr>,\n#   `Derecho a horas extraordinarias` <chr>, …\n```\n\n\n:::\n:::\n\n\n\n\nAhora que tenemos los datos, sólo resta guardarlos en nuestro proyecto de R y seguir procesándolos en otro script. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabla[[1]] |> readr::write_rds(\"datos.rds\")\n```\n:::\n\n\n\n\n\n### Descargar archivos\n\nFinalmente, tenemos la opción de descargar los datos de este sitio como un archivo en el botón _Descargar CSV_ que aparece arriba. Pero a diferencia de los enlaces, generalmente los botones de los sitios web no conllevan un enlace, sino que esperan que los aprietes para ejecutar un acción internamente que te entrega el archivo. En estos casos, sería imposible obtener el enlace del botón y descargar el archivo enlazado con `download.file(\"enlace\")`, Sino que simplemente hay que perder el botón y esperar que el sitio te entregue el archivo.\n\nUtilizamos las herramientas del navegador para identificar el `xpath` o selector CSS del botón para presionarlo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# descargar archivo\nremote$findElement('css selector', \n                   '.fa-file-csv')$\n  clickElement()\n```\n:::\n\n\n\n\nComo estamos controlando un navegador web real, el archivo descargado aparecerá en la **carpeta de descargas** de tu computadora, no en tu [proyecto de R](https://bastianolea.rbind.io/blog/r_introduccion/proyectos/). Pero podemos mover el archivo desde la carpeta de descargas a tu proyecto con el siguiente código:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fs)\n\n# crear una carpeta en tu proyecto\ndir_create(\"datos\")\n\n# mover archivo descargado al proyecto\nfile_move(path = \"~/Downloads/TransparenciaActiva.csv\",\n          new_path = \"datos/TransparenciaActiva.csv\")\n```\n:::\n\n\n\n\nOpcionalmente, al crear el navegador con `rsDriver()`, puedes configurar el navegador para especificar la ubicación de las descargas:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\n\nperfil <- makeFirefoxProfile(\n  list(browser.download.dir = here())\n)\n\nrsDriver(browser = \"firefox\", \n         chromever = NULL, phantomver = NULL,\n         extraCapabilities = perfil)\n```\n:::\n\n\n\n\n\n### Terminar la navegación\nCuándo termines de usar el navegador, tienes que cerrar la sesión. Esto se hace principalmente para liberar el puerto que asignaste al navegador al crearlo.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndriver$server$stop()\n```\n:::\n\n\n\n\n----\n\nPero te preguntarás, ¿por qué hicimos todo esto si desde el principio podíamos apretar el botón de descargas y obtener los datos? Lo primero es porque de esta manera tenemos un script con el que podemos seguir las instrucciones paso a paso para volver a descargar exactamente el mismo archivo en el futuro. Pero también, porque podemos usar este Skip para modificarlo y obtener un archivo de otra fecha, o incluso modificar la dirección de raíz del scraping y obtener el archivo equivalente pero de otro organismo público. \n\nEl potencial del web scraping no solamente obtener datos, sino obtenerlos de una manera tal que podamos automatizar la obtención masiva de los mismos.\n\n----\n\n### Otros\n\n#### Tomar captura de pantalla\n\nTambién es posible capturar lo que está mostrando el navegador por medio de una captura de pantalla:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$screenshot(file = 'pantallazo_1.jpg')\n```\n:::\n\n\n\n\n{{< imagen \"pantallazo_1.jpg\">}}\n\nEste tipo de acciones no es posible con herramientas de web scraping como `{rvest}`, que funcionan navegando directamente el código fuente del sitio, sin cargarlo como normalmente haría un navegador web.\n\n\n#### Desplazarse por el sitio\nOtro tipo de acciones que a veces son necesarias de hacer para obtener datos en un sitio es _scrollear_ por el mismo para que se carguen los elementos. Esto puede hacerse en Selenium ejecutando un script:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scrolear\nremote$executeScript(paste(\"window.scrollBy(0, \", 500, \");\"))\n\n# verificar visualmente\nremote$screenshot(file = 'pantallazo_2.jpg')\n```\n:::\n\n\n\n\n{{< imagen \"pantallazo_2.jpg\">}}\n\n\n#### Cambiar dimensiones de la ventana\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremote$setWindowSize(width = 640, height = 480)\n```\n:::\n\n\n\n\n#### Obtener dimensiones de la ventana\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nventana_alto <- remote$executeScript(\"return window.innerHeight\")[[1]]\nventana_ancho <- remote$executeScript(\"return window.innerWidth\")[[1]]\n```\n:::\n\n\n\n\n#### Grabar tu interacción con el navegador\nUsa [Selenium IDE](https://addons.mozilla.org/en-US/firefox/addon/selenium-ide/) para grabar interacciones con un sitio. Puedes instalar esta extensión de Firefox para _grabar_ tu interacción con un sitio web, y que queden registrado todos los pasos que hiciste en el sitio, de manera que puedas reproducirlos después en un script.\n\n----\n\n\n\n\n{{< cafecito >}}\n\n\n{{< cursos >}}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}